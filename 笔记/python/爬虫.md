源自：https://www.cnblogs.com/linhaifeng/p/8241221.html

# 第一篇：爬虫基本原理

## 一 爬虫是什么

```
#1、什么是互联网？
    互联网是由网络设备（网线，路由器，交换机，防火墙等等）和一台台计算机连接而成，像一张网一样。

#2、互联网建立的目的？
    互联网的核心价值在于数据的共享/传递：数据是存放于一台台计算机上的，而将计算机互联到一起的目的就是为了能够方便彼此之间的数据共享/传递，否则你只能拿U盘去别人的计算机上拷贝数据了。

#3、什么是上网？爬虫要做的是什么？
    我们所谓的上网便是由用户端计算机发送请求给目标计算机，将目标计算机的数据下载到本地的过程。
    #3.1 只不过，用户获取网络数据的方式是：
      浏览器提交请求->下载网页代码->解析/渲染成页面。

    #3.2 而爬虫程序要做的就是：
      模拟浏览器发送请求->下载网页代码->只提取有用的数据->存放于数据库或文件中
　
    #3.1与3.2的区别在于:
      我们的爬虫程序只提取网页代码中对我们有用的数据

#4、总结爬虫
    #4.1 爬虫的比喻：
      如果我们把互联网比作一张大的蜘蛛网，那一台计算机上的数据便是蜘蛛网上的一个猎物，而爬虫程序就是一只小蜘蛛，沿着蜘蛛网抓取自己想要的猎物/数据

    #4.2 爬虫的定义：
      向网站发起请求，获取资源后分析并提取有用数据的程序 

    #4.3 爬虫的价值：
      互联网中最有价值的便是数据，比如天猫商城的商品信息，链家网的租房信息，雪球网的证券投资信息等等，这些数据都代表了各个行业的真金白银，可以说，谁掌握了行业内的第一手数据，谁就成了整个行业的主宰，如果把整个互联网的数据比喻为一座宝藏，那我们的爬虫课程就是来教大家如何来高效地挖掘这些宝藏，掌握了爬虫技能，你就成了所有互联网信息公司幕后的老板，换言之，它们都在免费为你提供有价值的数据。
```



## 二 爬虫的基本流程

​	发送请求--->获取响应内容-->解析内容-->保存数据

```
#1、发起请求
使用http库向目标站点发起请求，即发送一个Request
Request包含：请求头、请求体等

#2、获取响应内容
如果服务器能正常响应，则会得到一个Response
Response包含：html，json，图片，视频等

#3、解析内容
解析html数据：正则表达式，第三方解析库如Beautifulsoup，pyquery等
解析json数据：json模块
解析二进制数据:以b的方式写入文件

#4、保存数据
数据库
文件
```

## 三 请求与响应

```
#http协议：http://www.cnblogs.com/linhaifeng/articles/8243379.html#Request：用户将自己的信息通过浏览器（socket client）发送给服务器（socket server）

#Response：服务器接收请求，分析用户发来的请求信息，然后返回数据（返回的数据中可能包含其他链接，如：图片，js，css等）

#ps：浏览器在接收Response后，会解析其内容来显示给用户，而爬虫程序在模拟浏览器发送请求然后接收Response后，是要提取其中的有用数据。
```

## 四 Request

```
#1、请求方式：
    常用的请求方式：GET，POST
    其他请求方式：HEAD，PUT，DELETE，OPTHONS

    ps：用浏览器演示get与post的区别，（用登录演示post）

    post与get请求最终都会拼接成这种形式：k1=xxx&k2=yyy&k3=zzz
    post请求的参数放在请求体内：
        可用浏览器查看，存放于form data内
    get请求的参数直接放在url后

#2、请求url
    url全称统一资源定位符，如一个网页文档，一张图片
    一个视频等都可以用url唯一来确定

    url编码
    https://www.baidu.com/s?wd=图片
    图片会被编码（看示例代码）


    网页的加载过程是：
    加载一个网页，通常都是先加载document文档，
    在解析document文档的时候，遇到链接，则针对超链接发起下载图片的请求

#3、请求头
    User-agent：请求头中如果没有user-agent客户端配置，
    服务端可能将你当做一个非法用户
    host
    cookies：cookie用来保存登录信息

    一般做爬虫都会加上请求头


#4、请求体
    如果是get方式，请求体没有内容
    如果是post方式，请求体是format data

    ps：
    1、登录窗口，文件上传等，信息都会被附加到请求体内
    2、登录，输入错误的用户名密码，然后提交，就可以看到post，正确登录后页面通常会跳转，无法捕捉到post 
```

## 五 Response

```
#1、响应状态
    200：代表成功
    301：代表跳转
    404：文件不存在
    403：权限
    502：服务器错误

#2、Respone header
    set-cookie：可能有多个，是来告诉浏览器，把cookie保存下来
    
#3、preview就是网页源代码
    最主要的部分，包含了请求资源的内容
    如网页html，图片
    二进制数据等
```

## 六 总结

```
#1、总结爬虫流程：
    爬取--->解析--->存储

#2、爬虫所需工具：
    请求库：requests,selenium
    解析库：正则，beautifulsoup，pyquery
    存储库：文件，MySQL，Mongodb，Redis

#3、爬虫常用框架：
    scrapy
```

# 第二篇：请求库之requests，selenium

## requests模块

### 一介绍requests

```
#介绍：使用requests可以模拟浏览器的请求，比起之前用到的urllib，requests模块的api更加便捷（本质就是封装了urllib3）

#注意：requests库发送请求将网页内容下载下来以后，并不会执行js代码，这需要我们自己分析目标站点然后发起新的request请求

#安装：pip3 install requests

#各种请求方式：常用的就是requests.get()和requests.post()
>>> import requests
>>> r = requests.get('https://api.github.com/events')
>>> r = requests.post('http://httpbin.org/post', data = {'key':'value'})
>>> r = requests.put('http://httpbin.org/put', data = {'key':'value'})
>>> r = requests.delete('http://httpbin.org/delete')
>>> r = requests.head('http://httpbin.org/get')
>>> r = requests.options('http://httpbin.org/get')

#建议在正式学习requests前，先熟悉下HTTP协议
http://www.cnblogs.com/linhaifeng/p/6266327.html
```

### 二 基于GET请求

**1、基本请求**

```
import requests
response=requests.get('http://dig.chouti.com/')
print(response.text)
```

2、带参数的GET请求

```
#在请求头内将自己伪装成浏览器，否则百度不会正常返回页面内容
import requests
response=requests.get('https://www.baidu.com/s?wd=python&pn=1',
                      headers={
                        'User-Agent':'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.75 Safari/537.36',
                      })
print(response.text)


#如果查询关键词是中文或者有其他特殊符号，则不得不进行url编码
from urllib.parse import urlencode
wd='egon老师'
encode_res=urlencode({'k':wd},encoding='utf-8')
keyword=encode_res.split('=')[1]
print(keyword)
# 然后拼接成url
url='https://www.baidu.com/s?wd=%s&pn=1' %keyword

response=requests.get(url,
                      headers={
                        'User-Agent':'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.75 Safari/537.36',
                      })
res1=response.text


#上述操作可以用requests模块的一个params参数搞定，本质还是调用urlencode
from urllib.parse import urlencode
wd='egon老师'
pn=1

response=requests.get('https://www.baidu.com/s',
                      params={
                          'wd':wd,
                          'pn':pn
                      },
                      headers={
                        'User-Agent':'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.75 Safari/537.36',
                      })
res2=response.text

#验证结果，打开a.html与b.html页面内容一样
with open('a.html','w',encoding='utf-8') as f:
    f.write(res1) 
with open('b.html', 'w', encoding='utf-8') as f:
    f.write(res2)
```

### 三 基于POST请求

**1、介绍**

```
#GET请求
HTTP默认的请求方法就是GET
     * 没有请求体
     * 数据必须在1K之内！
     * GET请求数据会暴露在浏览器的地址栏中

GET请求常用的操作：
       1. 在浏览器的地址栏中直接给出URL，那么就一定是GET请求
       2. 点击页面上的超链接也一定是GET请求
       3. 提交表单时，表单默认使用GET请求，但可以设置为POST


#POST请求
(1). 数据不会出现在地址栏中
(2). 数据的大小没有上限
(3). 有请求体
(4). 请求体中如果存在中文，会使用URL编码！
```











# 第三篇：解析库之re、beautifulsoup、pyquery

## 一 re正则

一：什么是正则？

　正则就是用一些具有特殊含义的符号组合到一起（称为正则表达式）来描述字符或者字符串的方法。或者说：正则就是用来描述一类事物的规则。（在Python中）它内嵌在Python中，并通过 re 模块实现。正则表达式模式被编译成一系列的字节码，然后由用 C 编写的匹配引擎执行。

（python基础.md）

## 二 beautifulsoup

### 一 介绍

Beautiful  Soup 是一个可以从HTML或XML文件中提取数据的Python库.它能够通过你喜欢的转换器实现惯用的文档导航,查找,修改文档的方式.Beautiful Soup会帮你节省数小时甚至数天的工作时间.你可能在寻找 Beautiful Soup3 的文档,Beautiful Soup 3  目前已经停止开发,官网推荐在现在的项目中使用Beautiful Soup 4, 移植到BS4

```
#安装 Beautiful Soup
pip install beautifulsoup4

#安装解析器
Beautiful Soup支持Python标准库中的HTML解析器,还支持一些第三方的解析器,其中一个是 lxml .根据操作系统不同,可以选择下列方法来安装lxml:

$ apt-get install Python-lxml

$ easy_install lxml

$ pip install lxml

另一个可供选择的解析器是纯Python实现的 html5lib , html5lib的解析方式与浏览器相同,可以选择下列方法来安装html5lib:

$ apt-get install Python-html5lib

$ easy_install html5lib

$ pip install html5lib
```

**下表列出了主要的解析器,以及它们的优缺点,官网推荐使用lxml作为解析器,因为效率更高. 在Python2.7.3之前的版本和Python3中3.2.2之前的版本,必须安装lxml或html5lib,  因为那些Python版本的标准库中内置的HTML解析方法不够稳定.**

| 解析器           | 使用方法                                                     | 优势                                                    | 劣势                                            |
| ---------------- | ------------------------------------------------------------ | ------------------------------------------------------- | ----------------------------------------------- |
| Python标准库     | `BeautifulSoup(markup, "html.parser")`                       | Python的内置标准库 执行速度适中 文档容错能力强          | Python 2.7.3 or 3.2.2)前 的版本中文档容错能力差 |
| lxml HTML 解析器 | `BeautifulSoup(markup, "lxml")`                              | 速度快 文档容错能力强                                   | 需要安装C语言库                                 |
| lxml XML 解析器  | `BeautifulSoup(markup, ["lxml", "xml"])` `BeautifulSoup(markup, "xml")` | 速度快 唯一支持XML的解析器                              | 需要安装C语言库                                 |
| html5lib         | `BeautifulSoup(markup, "html5lib")`                          | 最好的容错性 以浏览器的方式解析文档 生成HTML5格式的文档 | 速度慢 不依赖外部扩展                           |

### 二 基本使用

```
html_doc = """
<html><head><title>The Dormouse's story</title></head>
<body>
<p class="title"><b>The Dormouse's story</b></p>

<p class="story">Once upon a time there were three little sisters; and their names were
<a href="http://example.com/elsie" class="sister" id="link1">Elsie</a>,
<a href="http://example.com/lacie" class="sister" id="link2">Lacie</a> and
<a href="http://example.com/tillie" class="sister" id="link3">Tillie</a>;
and they lived at the bottom of a well.</p>

<p class="story">...</p>
"""

#基本使用：容错处理,文档的容错能力指的是在html代码不完整的情况下,使用该模块可以识别该错误。使用BeautifulSoup解析上述代码,能够得到一个 BeautifulSoup 的对象,并能按照标准的缩进格式的结构输出
from bs4 import BeautifulSoup
soup=BeautifulSoup(html_doc,'lxml') #具有容错功能
res=soup.prettify() #处理好缩进，结构化显示
print(res)
```



### 三 遍历文档树

```
#遍历文档树：即直接通过标签名字选择，特点是选择速度快，但如果存在多个相同的标签则只返回第一个
#1、用法
#2、获取标签的名称
#3、获取标签的属性
#4、获取标签的内容
#5、嵌套选择
#6、子节点、子孙节点
#7、父节点、祖先节点
#8、兄弟节点
```

### 六 总结

```
# 总结:
#1、推荐使用lxml解析库
#2、讲了三种选择器:标签选择器,find与find_all，css选择器
    1、标签选择器筛选功能弱,但是速度快
    2、建议使用find,find_all查询匹配单个结果或者多个结果
    3、如果对css选择器非常熟悉建议使用select
#3、记住常用的获取属性attrs和文本值get_text()的方法
```



# 第四篇：存储库之mongodb，redis，mysql

## 一 mongodb

### 一 简介

MongoDB是一款强大、灵活、且易于扩展的通用型数据库
1、易用性

```
MongoDB是一个面向文档（document-oriented）的数据库，而不是关系型数据库。不采用关系型主要是为了获得更好得扩展性。当然还有一些其他好处，与关系数据库相比，面向文档的数据库不再有“行“（row）的概念取而代之的是更为灵活的“文档”（document）模型。通过在文档中嵌入文档和数组，面向文档的方法能够仅使用一条记录来表现复杂的层级关系，这与现代的面向对象语言的开发者对数据的看法一致。
另外，不再有预定义模式（predefined schema）：文档的键（key）和值（value）不再是固定的类型和大小。由于没有固定的模式，根据需要添加或删除字段变得更容易了。通常由于开发者能够进行快速迭代，所以开发进程得以加快。而且，实验更容易进行。开发者能尝试大量的数据模型，从中选一个最好的。
```

2、易扩展性

```
应用程序数据集的大小正在以不可思议的速度增长。随着可用带宽的增长和存储器价格的下降，即使是一个小规模的应用程序，需要存储的数据量也可能大的惊人，甚至超出
了很多数据库的处理能力。过去非常罕见的T级数据，现在已经是司空见惯了。
由于需要存储的数据量不断增长，开发者面临一个问题：应该如何扩展数据库，分为纵向扩展和横向扩展，纵向扩展是最省力的做法，但缺点是大型机一般都非常贵，而且
当数据量达到机器的物理极限时，花再多的钱也买不到更强的机器了，此时选择横向扩展更为合适，但横向扩展带来的另外一个问题就是需要管理的机器太多。
MongoDB的设计采用横向扩展。面向文档的数据模型使它能很容易地在多台服务器之间进行数据分割。MongoDB能够自动处理跨集群的数据和负载，自动重新分配文档，以及将
用户的请求路由到正确的机器上。这样，开发者能够集中精力编写应用程序，而不需要考虑如何扩展的问题。如果一个集群需要更大的容量，只需要向集群添加新服务器，MongoDB就会自动将现有的数据向新服务器传送
```

3、丰富的功能

```
MongoDB作为一款通用型数据库，除了能够创建、读取、更新和删除数据之外，还提供了一系列不断扩展的独特功能
#1、索引
支持通用二级索引，允许多种快速查询，且提供唯一索引、复合索引、地理空间索引、全文索引

#2、聚合
支持聚合管道，用户能通过简单的片段创建复杂的集合，并通过数据库自动优化

#3、特殊的集合类型
支持存在时间有限的集合，适用于那些将在某个时刻过期的数据，如会话session。类似地，MongoDB也支持固定大小的集合，用于保存近期数据，如日志

#4、文件存储
支持一种非常易用的协议，用于存储大文件和文件元数据。MongoDB并不具备一些在关系型数据库中很普遍的功能，如链接join和复杂的多行事务。省略
这些的功能是处于架构上的考虑，或者说为了得到更好的扩展性，因为在分布式系统中这两个功能难以高效地实现
```

4、卓越的性能

```
MongoDB的一个主要目标是提供卓越的性能，这很大程度上决定了MongoDB的设计。MongoDB把尽可能多的内存用作缓存cache，视图为每次查询自动选择正确的索引。
总之各方面的设计都旨在保持它的高性能
虽然MongoDB非常强大并试图保留关系型数据库的很多特性，但它并不追求具备关系型数据库的所有功能。只要有可能，数据库服务器就会将处理逻辑交给客户端。这种精简方式的设计是MongoDB能够实现如此高性能的原因之一
```

### 二 MongoDB基础知识

![img](https://images2017.cnblogs.com/blog/1036857/201801/1036857-20180114154208504-1797989172.png)

1、文档是MongoDB的核心概念。文档就是键值对的一个有序集{'msg':'hello','foo':3}。类似于python中的有序字典。

```
需要注意的是：
#1、文档中的键/值对是有序的。
#2、文档中的值不仅可以是在双引号里面的字符串，还可以是其他几种数据类型（甚至可以是整个嵌入的文档)。
#3、MongoDB区分类型和大小写。
#4、MongoDB的文档不能有重复的键。
#5、文档中的值可以是多种不同的数据类型，也可以是一个完整的内嵌文档。文档的键是字符串。除了少数例外情况，键可以使用任意UTF-8字符。

文档键命名规范：
#1、键不能含有\0 (空字符)。这个字符用来表示键的结尾。
#2、.和$有特别的意义，只有在特定环境下才能使用。
#3、以下划线"_"开头的键是保留的(不是严格要求的)。
```

2、集合就是一组文档。如果将MongoDB中的一个文档比喻为关系型数据的一行，那么一个集合就是相当于一张表

```
#1、集合存在于数据库中，通常情况下为了方便管理，不同格式和类型的数据应该插入到不同的集合，但其实集合没有固定的结构，这意味着我们完全可以把不同格式和类型的数据统统插入一个集合中。

#2、组织子集合的方式就是使用“.”，分隔不同命名空间的子集合。
比如一个具有博客功能的应用可能包含两个集合，分别是blog.posts和blog.authors，这是为了使组织结构更清晰，这里的blog集合（这个集合甚至不需要存在）跟它的两个子集合没有任何关系。
在MongoDB中，使用子集合来组织数据非常高效，值得推荐

#3、当第一个文档插入时，集合就会被创建。合法的集合名：
集合名不能是空字符串""。
集合名不能含有\0字符（空字符)，这个字符表示集合名的结尾。
集合名不能以"system."开头，这是为系统集合保留的前缀。
用户创建的集合名字不能含有保留字符。有些驱动程序的确支持在集合名里面包含，这是因为某些系统生成的集合中包含该字符。除非你要访问这种系统创建的集合，否则千万不要在名字里出现$。
```

3、数据库：在MongoDB中，多个文档组成集合，多个集合可以组成数据库

```
数据库也通过名字来标识。数据库名可以是满足以下条件的任意UTF-8字符串：
#1、不能是空字符串（"")。
#2、不得含有' '（空格)、.、$、/、\和\0 (空字符)。
#3、应全部小写。
#4、最多64字节。

有一些数据库名是保留的，可以直接访问这些有特殊作用的数据库。
#1、admin： 从身份认证的角度讲，这是“root”数据库，如果将一个用户添加到admin数据库，这个用户将自动获得所有数据库的权限。再者，一些特定的服务器端命令也只能从admin数据库运行，如列出所有数据库或关闭服务器
#2、local: 这个数据库永远都不可以复制，且一台服务器上的所有本地集合都可以存储在这个数据库中
#3、config: MongoDB用于分片设置时，分片信息会存储在config数据库中
```

4、强调：把数据库名添加到集合名前，得到集合的完全限定名，即命名空间

```
例如：如果要使用cms数据库中的blog.posts集合，这个集合的命名空间就是
cmd.blog.posts。命名空间的长度不得超过121个字节，且在实际使用中应该小于100个字节
```

### 五 CRUD操作

1、数据库操作

```
#1、增
use config #如果数据库不存在，则创建数据库，否则切换到指定数据库。

#2、查
show dbs #查看所有
可以看到，我们刚创建的数据库config并不在数据库的列表中， 要显示它，我们需要向config数据库插入一些数据。
db.table1.insert({'a':1})

#3、删
use config #先切换到要删的库下
db.dropDatabase() #删除当前库
```









# 第五篇：爬虫高性能相关









# 第六篇：Scrapy框架







# 第七篇：分布式爬虫









# 第八篇：爬虫实战







